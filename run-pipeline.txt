run architecture

kubectl get pods

data sources !!!
make kubectl-database-k8s
make kubectl-running-database-k8s
make DDL
make DML
kubectl scale deployment postgres --replicas=1
kubectl scale deployment postgres --replicas=0

--------------------------------------------------------------------------------------------------------------------------------------

data streaming !!!
run pertama:
make kubectl-streaming-k8s

run kedua:
make kubectl-running-streaming-debezium-k8s

run bareng stop/delete:
make kubectl-Stop-Streaming-k8s

run container debezium:
make kubectl-running-streaming-debezium-k8s

--------------------------------------------------------------------------------------------------------------------------------------

data batching !!!
make kubectl-batching-k8s
make kubectl-Starting-batching-k8s
make kubectl-Stopping-batching-k8s

---------------------------------------------------------------------------------------------------------------------------------------

kubectl get pods

kubectl exec -it flink-jobmanager-xxxxxxxxxx -- bash

copy flink :
kubectl cp flink-scripts/flink_consumer.py flink-jobmanager-xxxxxxxxxx:/opt/flink/flink-scripts/flink_consumer.py 

run flink normal :

flink run -py /opt/flink/flink-scripts/flink_consumer.py -p 1

run flink backfill (BACKFILL_FROM_DATE = diubah sesuai tanggal kebutuhan):

FLINK_MODE=backfill BACKFILL_FROM_DATE=2026-01-28 flink run -py /opt/flink/flink-scripts/flink_consumer.py -p 1

spark bash :

kubectl exec -it spark-xxxxxxxxxx -- bash

copy aws :
kubectl cp spark-scripts/spark_jobs_s3.py cek_nods:/opt/app/spark_jobs_s3.py

1.spark+aws :
mkdir secret_aws

2.copy spark_aws :
kubectl cp .env cek_nods:/opt/app/.env

3.spark+aws (penting) in /opt/app/ :
set -a source .env set +a

output env :
cat /opt/app/.env

run spark :

spark-submit spark_jobs_s3.py --run_all # run all

spark-submit spark_jobs_s3.py --table orders # per table

copy file/data ke local :
kubectl cp spark-849c654cfc-mfqdl:/opt/app/output/bronze_csv ./bronze_csv

spark-submit spark_jobs_s3.py --table customer --process_date 2026-01-21--06

spark-submit Ingest_to_Redshift.py --table customer --process_date 2026-01-21


spark-submit --packages org.apache.hadoop:hadoop-aws:3.3.4 --jars /path/to/RedshiftJDBC42-no-awssdk-2.1.0.12.jar Ingestion_to_Redshift.py --table customer

spark-submit Ingestion_to_Redshift.py --table customer --process_date 2026-01-21

---------------------------------------------------------------------------------------------------------------------------------------

DDL in Redshift :

CREATE SCHEMA IF NOT EXISTS bronze;
CREATE SCHEMA IF NOT EXISTS silver;
CREATE SCHEMA IF NOT EXISTS gold;

run cdc lagi ke kafka lalu ke batching


---------------------------------------------------------------------------------------------------------------------------------------

export REDSHIFT_HOST=real-time-ecommerce-analytics-cluster.cqx6yxf2ywld.ap-southeast-1.redshift.amazonaws.com
export REDSHIFT_DB=real_time_ecommerce_analytics
export REDSHIFT_USER=adminuser
export REDSHIFT_PASSWORD='Alonedc35!'
export REDSHIFT_PORT=5439

---------------------------------------------------------------------------------------------------------------------------------------

flink narik data bedasarkan current date atau bisa di backfill sesuai tanggal kebutuhan
buat ingestion ke spark tanpa duplikat 
main airflow