- run architecture

kubectl get pods

- data sources !!!

make kubectl-database-k8s
make kubectl-running-database-k8s
make DDL
make DML
kubectl scale deployment postgres --replicas=1
kubectl scale deployment postgres --replicas=0

--------------------------------------------------------------------------------------------------------------------------------------

- data streaming !!!

- run pertama:

make kubectl-streaming-k8s

- run kedua:

make kubectl-running-streaming-debezium-k8s

- run bareng stop/delete:

make kubectl-Stop-Streaming-k8s

- run container debezium:

make kubectl-running-streaming-debezium-k8s

--------------------------------------------------------------------------------------------------------------------------------------

- data batching !!!

make kubectl-batching-k8s
make kubectl-Starting-batching-k8s
make kubectl-Stopping-batching-k8s

---------------------------------------------------------------------------------------------------------------------------------------

- proses streaming !!!!

kubectl get pods

kubectl exec -it flink-jobmanager-xxxxxxxxxx -- bash

- copy flink :

kubectl cp flink-scripts/flink_consumer.py flink-jobmanager-xxxxxxxxxx:/opt/flink/flink-scripts/flink_consumer.py 

- run flink normal (secara real-time):

flink run -py /opt/flink/flink-scripts/flink_consumer.py -p 1

- run flink backfill (BACKFILL_FROM_DATE = diubah sesuai tanggal kebutuhan):

FLINK_MODE=backfill BACKFILL_FROM_DATE=2026-01-28 flink run -py /opt/flink/flink-scripts/flink_consumer.py -p 1

---------------------------------------------------------------------------------------------------------------------------------------

- proses batching !!!

- spark bash :

kubectl exec -it spark-xxxxxxxxxx -- bash

- copy spark_jobs_s3 :

kubectl cp spark-scripts/spark_jobs_s3.py cek_nods:/opt/app/spark_jobs_s3.py

- copy spark_aws .env :

kubectl cp .env cek_nods:/opt/app/.env

- spark+aws (penting) in /opt/app/ :

set -a source .env set +a

- output env :

cat .env

- copy file/data ke local :

kubectl cp spark-849c654cfc-mfqdl:/opt/app/output/bronze_csv ./bronze_csv

- run to s3 change parquet :

spark-submit spark_jobs_s3.py --run_all #ambil semua

spark-submit spark_jobs_s3.py --table customer --process_date 2026-01-21--06 #backfill_date

- run to redshift :

spark-submit Ingestion_to_Redshift.py --table customer --process_date 2026-01-21 #backfill_date

REDSHIFT_HOST=xxxxxxxxxx REDSHIFT_DB=xxxxxxxxxx REDSHIFT_USER=xxxxxxxxxx REDSHIFT_PASSWORD=xxxxxxxxxx REDSHIFT_PORT=5439 spark-submit Ingestion_to_Redshift.py --table customer --process_date 2026-01-21 #backfill_date2

spark-submit --packages org.apache.hadoop:hadoop-aws:3.3.4 --jars /path/to/RedshiftJDBC42-no-awssdk-2.1.0.12.jar Ingestion_to_Redshift.py --table customer

---------------------------------------------------------------------------------------------------------------------------------------

DDL in Redshift :

CREATE SCHEMA IF NOT EXISTS bronze;
CREATE SCHEMA IF NOT EXISTS silver;
CREATE SCHEMA IF NOT EXISTS gold;

run cdc lagi ke kafka lalu ke batching


---------------------------------------------------------------------------------------------------------------------------------------

saat mau ingestion ke redshift :

export REDSHIFT_HOST=xxxxxxxxxx
export REDSHIFT_DB=xxxxxxxxxx
export REDSHIFT_USER=xxxxxxxxxx
export REDSHIFT_PASSWORD=xxxxxxxxxx
export REDSHIFT_PORT=5439

---------------------------------------------------------------------------------------------------------------------------------------

flink narik data bedasarkan current date atau bisa di backfill sesuai tanggal kebutuhan
buat ingestion ke spark tanpa duplikat 
main airflow