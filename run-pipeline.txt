- run architecture

kubectl get pods

- data sources !!!

make kubectl-database-k8s
make kubectl-running-database-k8s
make DDL
make DML
kubectl scale deployment postgres --replicas=1
kubectl scale deployment postgres --replicas=0

--------------------------------------------------------------------------------------------------------------------------------------

- data streaming !!!

- run pertama:

make kubectl-streaming-k8s

- run kedua:

make kubectl-running-streaming-debezium-k8s

- run bareng stop/delete:

make kubectl-Stop-Streaming-k8s

- run container debezium:

make kubectl-running-streaming-debezium-k8s

--------------------------------------------------------------------------------------------------------------------------------------

- data batching !!!

make kubectl-batching-k8s
make kubectl-Starting-batching-k8s
make kubectl-Stopping-batching-k8s

---------------------------------------------------------------------------------------------------------------------------------------

- monitoring !!!

make kubectl-monitoring-k8s
make kubectl-Stopping-monitoring-k8s
make kubectl-Starting-monitoring-k8s

---------------------------------------------------------------------------------------------------------------------------------------

- proses streaming !!!!

kubectl get pods

kubectl exec -it flink-jobmanager-xxxxxxxxxx -- bash

- copy flink :

kubectl cp flink-scripts/flink_consumer.py flink-jobmanager-xxxxxxxxxx:/opt/flink/flink-scripts/flink_consumer.py 

- run flink normal (secara real-time):

flink run -py /opt/flink/flink-scripts/flink_consumer.py -p 1

- run flink backfill (BACKFILL_FROM_DATE = diubah sesuai tanggal kebutuhan):

FLINK_MODE=backfill BACKFILL_FROM_DATE=2026-01-28 flink run -py /opt/flink/flink-scripts/flink_consumer.py -p 1

---------------------------------------------------------------------------------------------------------------------------------------

- proses batching !!!

- spark bash :

kubectl exec -it spark-xxxxxxxxxx -- bash

- copy spark_jobs_s3 :

kubectl cp spark-scripts/spark_jobs_s3.py cek_nods:/opt/app/spark_jobs_s3.py

- copy Ingestion_to_Redshift Ingestion_to_Redshift :

kubectl cp spark-scripts/Ingestion_to_Redshift.py cek_nods:/opt/app/Ingestion_to_Redshift.py

- copy Ingestion_per-table :

kubectl cp spark-scripts/Ingestion_Customer.py cek_nods:/opt/app/Ingestion_Customer.py
kubectl cp spark-scripts/Ingestion_Orders.py cek_nods:/opt/app/Ingestion_Orders.py
kubectl cp spark-scripts/Ingestion_Payments.py cek_nods:/opt/app/Ingestion_Payments.py
kubectl cp spark-scripts/Ingestion_Product.py cek_nods:/opt/app/Ingestion_Product.py
kubectl cp spark-scripts/Ingestion_Shipping.py cek_nods:/opt/app/Ingestion_Shipping.py

- copy spark_aws .env :

kubectl cp .env cek_nods:/opt/app/.env

- spark+aws (penting) in /opt/app/ :

set -a source .env set +a

- output env :

cat .env

- copy file/data ke local :

kubectl cp spark-849c654cfc-mfqdl:/opt/app/output/bronze_csv ./bronze_csv

- run to s3 change parquet :

spark-submit spark_jobs_s3.py --run_all #ambil semua

spark-submit spark_jobs_s3.py --table customer --process_date 2026-01-21--06 #backfill_date

- run to redshift Ingestion_to_Redshift :

spark-submit Ingestion_to_Redshift.py --table customer --process_date 2026-01-21 #backfill_date

- run to redshift Ingestion_per-table :

spark-submit Ingestion_Customer.py --process_date 2026-01-28 #backfill_date_pertable
spark-submit Ingestion_Orders.py --process_date 2026-01-28 #backfill_date_pertable
spark-submit Ingestion_Payments.py --process_date 2026-01-28 #backfill_date_pertable
spark-submit Ingestion_Product.py --process_date 2026-01-28 #backfill_date_pertable
spark-submit Ingestion_Shipping.py --process_date 2026-01-28 #backfill_date_pertable

spark-submit Ingestion_Customer.py --run_all
spark-submit Ingestion_Orders.py --run_all
spark-submit Ingestion_Payments.py --run_all
spark-submit Ingestion_Product.py --run_all
spark-submit Ingestion_Shipping.py --run_all

spark-submit Ingestion_Customer.py --run_latest
spark-submit Ingestion_Orders.py --run_latest
spark-submit Ingestion_Payments.py --run_latest
spark-submit Ingestion_Product.py --run_latest
spark-submit Ingestion_Shipping.py --run_latest

---------------------------------------------------------------------------------------------------------------------------------------

DDL in Redshift :

CREATE SCHEMA IF NOT EXISTS bronze;
CREATE SCHEMA IF NOT EXISTS silver;
CREATE SCHEMA IF NOT EXISTS gold;

run cdc lagi ke kafka lalu ke batching


---------------------------------------------------------------------------------------------------------------------------------------

saat mau ingestion ke redshift :

export REDSHIFT_HOST=xxxxxxxxxx
export REDSHIFT_DB=xxxxxxxxxx
export REDSHIFT_USER=xxxxxxxxxx
export REDSHIFT_PASSWORD=xxxxxxxxxx
export REDSHIFT_PORT=5439

---------------------------------------------------------------------------------------------------------------------------------------

flink narik data bedasarkan current date atau bisa di backfill sesuai tanggal kebutuhan
buat ingestion ke spark tanpa duplikat 
main airflow