x-airflow-common: &airflow-common
  image: batching_airflow
  environment: &airflow-common-env
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_CONTAINER_NAME}/${POSTGRES_DB}
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_CONTAINER_NAME}/${POSTGRES_DB}
    AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-webserver:8080/execution/'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: 60
    AIRFLOW__CORE__ENABLE_REACT_UI: 'true'
    AIRFLOW__WEBSERVER__RELOAD_ON_PLUGIN_CHANGE: 'true'
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
    AIRFLOW__CORE__LAZY_LOAD_PLUGINS: 'false'
    _AIRFLOW_DB_UPGRADE: 'true'
    _AIRFLOW_DB_MIGRATE: 'true'
    _AIRFLOW_WWW_USER_CREATE: 'true'
    _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_WWW_USER_USERNAME}
    _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_WWW_USER_PASSWORD}
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    AIRFLOW_UID: 50000
    POSTGRES_USER: ${POSTGRES_USER}
    POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    POSTGRES_DB: ${POSTGRES_DB}
    POSTGRES_CONTAINER_NAME: ${POSTGRES_CONTAINER_NAME}
    POSTGRES_PORT: ${POSTGRES_PORT}
    SPARK_MASTER_HOST_NAME: ${SPARK_MASTER_HOST_NAME}
    SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
    AIRFLOW__METRICS__STATSD_PREFIX: airflow
    AIRFLOW__METRICS__STATSD_ON: 'True'
    AIRFLOW__METRICS__ENABLED: 'True'
  user: "${AIRFLOW_UID:-50000}:0"
  volumes:
    - airflow_dags:/opt/airflow/dags
    - airflow_logs:/opt/airflow/logs
    - airflow_dbt:/opt/airflow/.dbt
    - ../spark-scripts:/spark-scripts
    - ../data:/data
    - ../scripts:/scripts
    - ${HOME}/.GCP:/root/.GCP
    - ${HOME}/.AWS:/root/.AWS

services:
  airflow-scheduler:
    <<: *airflow-common
    container_name: ${AIRFLOW_SCHEDULER_CONTAINER_NAME}
    hostname: ${AIRFLOW_SCHEDULER_CONTAINER_NAME}
    command: scheduler
    restart: always
    environment:
      <<: *airflow-common-env

  airflow-apiserver:
    <<: *airflow-common
    container_name: ${AIRFLOW_APISERVER_CONTAINER_NAME}
    hostname: ${AIRFLOW_APISERVER_CONTAINER_NAME}
    entrypoint: /scripts/entrypoint.sh
    command: api-server
    restart: always
    environment:
      <<: *airflow-common-env
    ports:
      - ${AIRFLOW_APISERVER_PORT}:8080
    depends_on:
      - airflow-scheduler

volumes:
  airflow_dags:
    name: airflow_dags
  airflow_logs:
    name: airflow_logs
  airflow_dbt:
    name: airflow_dbt

networks:
  default:
    name: Batching-Processing
    external: true